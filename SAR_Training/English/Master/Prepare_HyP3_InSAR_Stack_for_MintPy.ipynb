{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hidePrompt": true
   },
   "source": [
    "<img src=\"NotebookAddons/blackboard-banner.png\" width=\"100%\" />\n",
    "\n",
    "# Preparing a HyP3 InSAR Stack for MintPy\n",
    "\n",
    "**Author**: Alex Lewandowski; University of Alaska Fairbanks\n",
    " \n",
    "Based on [prep_hyp3_for_mintpy.ipynb](https://github.com/ASFHyP3/hyp3-docs) by Jiang Zhu; University of Alaska Fairbanks\n",
    "\n",
    "<img src=\"NotebookAddons/UAFLogo_A_647.png\" width=\"170\" align=\"right\" />\n",
    "\n",
    "**Important Note about JupyterHub**\n",
    "Your JupyterHub server will automatically shutdown when left idle for more than 1 hour. Your notebooks will not be lost but you will have to restart their kernels and re-run them from the beginning. You will not be able to seamlessly continue running a partially run notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%javascript\n",
    "var kernel = Jupyter.notebook.kernel;\n",
    "var command = [\"notebookUrl = \",\n",
    "               \"'\", window.location, \"'\" ].join('')\n",
    "kernel.execute(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "user = !echo $JUPYTERHUB_USER\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/insar_analysis':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"insar_analysis\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select the \"insar_analysis\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"insar_analysis\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing Relevant Python Packages\n",
    "\n",
    "In this notebook we will use the following scientific libraries:\n",
    "- [GDAL](https://www.gdal.org/) is a software library for reading and writing raster and vector geospatial data formats. It includes a collection of programs tailored for geospatial data processing. Most modern GIS systems (such as ArcGIS or QGIS) use GDAL in the background.\n",
    "\n",
    "**Our first step is to import gdal and other needed packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "import ipywidgets as widgets\n",
    "from itertools import chain\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from typing import Union\n",
    "\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import pyproj\n",
    "from pyproj import Transformer\n",
    "import pandas\n",
    "\n",
    "import asf_notebook as asfn\n",
    "\n",
    "from hyp3_sdk import Batch, HyP3\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code in this cell is taken directly from the develop branch of asf_tools, \n",
    "# https://github.com/ASFHyP3/asf-tools\n",
    "\n",
    "# Delete this cell, install, and import asf-tools when v0.3.0 is released:\n",
    "# https://github.com/ASFHyP3/asf-tools/pull/90\n",
    "\n",
    "\n",
    "from osgeo import gdal\n",
    "\n",
    "gdal.UseExceptions()\n",
    "\n",
    "\n",
    "class GDALConfigManager:\n",
    "    \"\"\"Context manager for setting GDAL config options temporarily\"\"\"\n",
    "    def __init__(self, **options):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            **options: GDAL Config `option=value` keyword arguments.\n",
    "        \"\"\"\n",
    "        self.options = options.copy()\n",
    "        self._previous_options = {}\n",
    "\n",
    "    def __enter__(self):\n",
    "        for key in self.options:\n",
    "            self._previous_options[key] = gdal.GetConfigOption(key)\n",
    "\n",
    "        for key, value in self.options.items():\n",
    "            gdal.SetConfigOption(key, value)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        for key, value in self._previous_options.items():\n",
    "            gdal.SetConfigOption(key, value)\n",
    "            \n",
    "            \n",
    "from pathlib import Path\n",
    "from typing import Iterator, List, Union\n",
    "\n",
    "from osgeo import ogr\n",
    "\n",
    "ogr.UseExceptions()\n",
    "\n",
    "\n",
    "def get_features(vector_path: Union[str, Path]) -> List[ogr.Feature]:\n",
    "    ds = ogr.Open(str(vector_path))\n",
    "    layer = ds.GetLayer()\n",
    "    return [feature for feature in layer]\n",
    "\n",
    "\n",
    "def get_property_values_for_intersecting_features(geometry: ogr.Geometry, features: Iterator) -> bool:\n",
    "    for feature in features:\n",
    "        if feature.GetGeometryRef().Intersects(geometry):\n",
    "            return True\n",
    "\n",
    "\n",
    "def intersecting_feature_properties(geometry: ogr.Geometry, features: Iterator,\n",
    "                                    feature_property: str) -> List[str]:\n",
    "    property_values = []\n",
    "    for feature in features:\n",
    "        if feature.GetGeometryRef().Intersects(geometry):\n",
    "            property_values.append(feature.GetField(feature_property))\n",
    "    return property_values\n",
    "\n",
    "\n",
    "\"\"\"Prepare a Copernicus GLO-30 DEM virtual raster (VRT) covering a given geometry\"\"\"\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "from osgeo import gdal, ogr\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "\n",
    "DEM_GEOJSON = '/vsicurl/https://asf-dem-west.s3.amazonaws.com/v2/cop30.geojson'\n",
    "\n",
    "gdal.UseExceptions()\n",
    "ogr.UseExceptions()\n",
    "\n",
    "\n",
    "def prepare_dem_vrt(vrt: Union[str, Path], geometry: Union[ogr.Geometry, BaseGeometry]):\n",
    "    \"\"\"Create a DEM mosaic VRT covering a given geometry\n",
    "    The DEM mosaic is assembled from the Copernicus GLO-30 DEM tiles that intersect the geometry.\n",
    "    Note: `asf_tools` does not currently support geometries that cross the antimeridian.\n",
    "    Args:\n",
    "        vrt: Path for the output VRT file\n",
    "        geometry: Geometry in EPSG:4326 (lon/lat) projection for which to prepare a DEM mosaic\n",
    "    \"\"\"\n",
    "    with GDALConfigManager(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR'):\n",
    "        if isinstance(geometry, BaseGeometry):\n",
    "            geometry = ogr.CreateGeometryFromWkb(geometry.wkb)\n",
    "\n",
    "        min_lon, max_lon, _, _ = geometry.GetEnvelope()\n",
    "        if min_lon < -160. and max_lon > 160.:\n",
    "            raise ValueError(f'asf_tools does not currently support geometries that cross the antimeridian: {geometry}')\n",
    "\n",
    "        tile_features = get_features(DEM_GEOJSON)\n",
    "        if not get_property_values_for_intersecting_features(geometry, tile_features):\n",
    "            raise ValueError(f'Copernicus GLO-30 DEM does not intersect this geometry: {geometry}')\n",
    "\n",
    "        dem_file_paths = intersecting_feature_properties(geometry, tile_features, 'file_path')\n",
    "\n",
    "        gdal.BuildVRT(str(vrt), dem_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Your Own Data Stack Into the Notebook\n",
    "\n",
    "This notebook assumes that you are accessing an InSAR time series created using the [Alaska Satellite Facility's](https://www.asf.alaska.edu/) value-added product system HyP3, available via [ASF Data Search/Vertex](https://search.asf.alaska.edu/). HyP3 is an ASF service used to prototype value added products and provide them to users to collect feedback.\n",
    "\n",
    "You can access HyP3 on-demand products from your HyP3 account or from publically available, pre-processed SARVIEWS Event data. https://sarviews-hazards.alaska.edu/\n",
    "\n",
    "Before downloading anything, create an analysis directory to hold your data.\n",
    "\n",
    "**Select or create a working directory for the analysis:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    data_dir = Path(asfn.input_path(f\"\\nPlease enter the name of a directory in which to store your data for this analysis.\"))\n",
    "    if data_dir == Path('.'):\n",
    "        continue\n",
    "    if data_dir.is_dir():\n",
    "        contents = data_dir.glob('*')\n",
    "        if len(list(contents)) > 0:\n",
    "            choice = asfn.handle_old_data(data_dir, list(contents))\n",
    "            if choice == 1:\n",
    "                shutil.rmtree(data_dir)\n",
    "                data_dir.mkdir()\n",
    "                break\n",
    "            elif choice == 2:\n",
    "                break\n",
    "            else:\n",
    "                clear_output()\n",
    "                continue\n",
    "        else:\n",
    "            break\n",
    "    else:\n",
    "        data_dir.mkdir()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define absolute path to  analysis directory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_directory = Path.cwd()/(data_dir)\n",
    "print(f\"analysis_directory: {analysis_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decide whether to pull data from HyP3 or SARVIEWS:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Are you downloading a HyP3 project or SARVIEWS event?\")\n",
    "data_source = asfn.select_parameter(['HyP3', 'SARVIEWS'], '')\n",
    "display(data_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create some data source flags for use in the notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp3_data = data_source.value == 'HyP3'\n",
    "sarviews_data = data_source.value == 'SARVIEWS'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select a SARVIEWS event type (or skip ahead to the HyP3 section):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sarviews_data:\n",
    "    event_type = asfn.select_parameter(['quake', 'volcano'], '')\n",
    "    print(\"Select an event type\")\n",
    "    display(event_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select a SARVIEWS Event:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sarviews_data:\n",
    "    sarviews_api = 'https://gm3385dq6j.execute-api.us-west-2.amazonaws.com/events'\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            sarviews_api,\n",
    "            params=[('output', 'json')]\n",
    "        )\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        if len(response.json()) > 0:\n",
    "            json_response = response.json()\n",
    "        else:\n",
    "            print(\"SARVIEWS returned no events\")\n",
    "            \n",
    "    events = [ i for i in json_response if i['event_type'] == event_type.value]\n",
    "    if event_type.value == 'quake':\n",
    "        event_names = asfn.select_parameter([f\"{e['description']} (depth: {e['depth']}, magnitude: {e['magnitude']})\" for e in events], '')\n",
    "    else:\n",
    "        event_names = asfn.select_parameter([e['description'] for e in events], '')\n",
    "    print(f\"Select a {event_type.value} event\")\n",
    "    display(event_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather the paths to your event's InSAR products from the SARVIEWS s3 bucket:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sarviews_data:\n",
    "    event = [e for e in json_response if e['description'] == event_names.value.split(' (')[0]][0]\n",
    "    print(f\"Event Details: \\n{event}\\n\")\n",
    "    event_id = event['event_id']\n",
    "    \n",
    "    s3_path = \"s3://hyp3-event-monitoring-productbucket-1t80jdtrfscje/\"\n",
    "    s3_event_path = f\"{s3_path}{event_id}/\"\n",
    "    job_dirs = !aws --no-sign-request --region us-west-2 s3 ls $s3_event_path\n",
    "    jobs = !aws --no-sign-request --region us-west-2 s3 ls $s3_event_path --recursive | grep .zip\n",
    "    jobs = [job.split(' ')[-1] for job in jobs if 'INT' in job]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter products by date range:**\n",
    "\n",
    "Some SARVIEWS Events last years and are too large to download to OpenSARlab in their entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dates_from_product_name(product_name):\n",
    "    regex = \"[0-9]{8}T[0-9]{6}_[0-9]{8}T[0-9]{6}\"\n",
    "    results = re.search(regex, product_name)\n",
    "    if results:\n",
    "        return results.group(0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if sarviews_data:\n",
    "    dates = set()\n",
    "    for j in jobs:\n",
    "        job_dates = dates_from_product_name(j).split('_')\n",
    "        dates.add(datetime.strptime(job_dates[0], '%Y%m%dT%H%M%S').date())\n",
    "        dates.add(datetime.strptime(job_dates[1], '%Y%m%dT%H%M%S').date()) \n",
    "    dates = list(dates)\n",
    "    dates.sort()\n",
    "    date_picker = selection_range_slider = widgets.SelectionRangeSlider(\n",
    "    options = dates,\n",
    "    index = (0, len(dates)-1),\n",
    "    description = 'Dates',\n",
    "    orientation = 'horizontal',\n",
    "    layout = {'width': '500px'})       \n",
    "\n",
    "    display(date_picker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the number of jobs found in your date range:**\n",
    "\n",
    "The number may be high, which is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sarviews_data:\n",
    "    filtered_jobs = [job.split(' ')[-1] for job in jobs if datetime.strptime(asfn.date_from_product_name(job).split('T')[0], \"%Y%m%d\").date() >= date_picker.value[0] and datetime.strptime(asfn.date_from_product_name(job).split('T')[0], \"%Y%m%d\").date() <= date_picker.value[1]]\n",
    "    \n",
    "    print(f\"Found {len(filtered_jobs)} jobs, some of which may belong to temporally disconnected stacks.\\n\")\n",
    "    print(f\"We will now sort the jobs into connected stacks and select one to download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identify all temporally connected stacks in your date range:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if sarviews_data:\n",
    "    \n",
    "    def get_start_end_stack_dates(stack):\n",
    "        dates = list(chain.from_iterable((dates_from_product_name(str(s)).split('_')[0], dates_from_product_name(str(s)).split('_')[1]) for s in stack))\n",
    "        dates.sort()\n",
    "        return datetime.strptime(dates[0], '%Y%m%dT%H%M%S').date(), datetime.strptime(dates[-1], '%Y%m%dT%H%M%S').date()\n",
    "\n",
    "\n",
    "    def get_temporal_stacks(jobs):\n",
    "        stacks = dict()\n",
    "        job_dates = {k: dates_from_product_name(k).split('_') for k in jobs}\n",
    "        stack_name = 0\n",
    "        for j in job_dates:\n",
    "            if len(stacks) == 0:\n",
    "                stacks.update({stack_name: [j]})\n",
    "                stack_name += 1\n",
    "                continue\n",
    "            stacks_copy = copy.copy(stacks)\n",
    "            added = False\n",
    "            for s in stacks_copy:\n",
    "                stack_dates = list(chain.from_iterable((dates_from_product_name(i).split('_')[0],\n",
    "                                                       dates_from_product_name(i).split('_')[1]) for i in stacks[s]))\n",
    "                if job_dates[j][0] in stack_dates or job_dates[j][1] in stack_dates:\n",
    "                    stacks[s].append(j)\n",
    "                    added = True\n",
    "            if not added:\n",
    "                stacks.update({stack_name: [j]})\n",
    "                stack_name += 1\n",
    "        return stacks\n",
    "    \n",
    "    stacks = get_temporal_stacks(filtered_jobs)\n",
    "    print(f\"Found {len(stacks)} stacks.\\n\")\n",
    "    for s in stacks:\n",
    "        start, end = get_start_end_stack_dates(stacks[s])\n",
    "        print(f\"TEMPORAL STACK #{s}, {len(stacks[s])} Scenes\")\n",
    "        print(f\"Timespan: {start} - {end}\")\n",
    "        for j in stacks[s]:\n",
    "            print(j.split('/')[-1])\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select a temporally connected stack to download:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sarviews_data:\n",
    "    stack = asfn.select_parameter(stacks, '')\n",
    "    print(\"Select a temporally connected stack to download:\")\n",
    "    display(stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confirm you have space to download your data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sarviews_data:\n",
    "    print(f\"Found {len(stack.value)} jobs\\n\")\n",
    "    print(f\"Confirm that you have space to store {len(stack.value)} jobs or you will\")\n",
    "    print(\"risk overrunning your volume capacity and be locked out of OpenSARlab.\\n\")\n",
    "    print(f\"If you become locked out of OpenSARlab, please contact an administrator for help.\\n\\n\")\n",
    "    %df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download and unzip the interferograms:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sarviews_data:\n",
    "    for j in tqdm(stack.value):\n",
    "        print(j)\n",
    "        source = f\"{s3_path}{j}\"\n",
    "        dest = Path(f\"{analysis_directory}/{j.split('/')[-1]}\")\n",
    "        !aws --no-sign-request --region us-west-2 s3 cp $source $dest\n",
    "     \n",
    "        asfn.asf_unzip(str(analysis_directory), str(dest))\n",
    "        dest.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download a DEM and align it with the time series:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sarviews_data:\n",
    "    fnames = list(analysis_directory.glob(f'*/*.tif'))\n",
    "    fnames.sort()\n",
    "\n",
    "    # Find the largest AOI intersecting any image\n",
    "    corners = [gdal.Info(str(f), format='json')['cornerCoordinates'] for f in fnames]\n",
    "    ulx = min(corner['upperLeft'][0] for corner in corners)\n",
    "    uly = max(corner['upperLeft'][1] for corner in corners)\n",
    "    lrx = max(corner['lowerRight'][0] for corner in corners)\n",
    "    lry = min(corner['lowerRight'][1] for corner in corners)\n",
    "\n",
    "    # corner coords to latlon for prepare_dem_vrt()\n",
    "    resolution = int(gdal.Info(str(fnames[0]), format='json')['geoTransform'][1])\n",
    "    info = gdal.Info(str(fnames[0]), format='json')['coordinateSystem']['wkt']\n",
    "    utm = info.split('ID')[-1].split(',')[1][0:-2]\n",
    "    transformer = Transformer.from_crs(f\"epsg:{utm}\", \"epsg:4326\")    \n",
    "    ul = transformer.transform(ulx, uly)\n",
    "    lr = transformer.transform(lrx, lry)\n",
    "\n",
    "    # create osgeo Geometry object of AOI \n",
    "    geojson = {\n",
    "        'type': 'Polygon',\n",
    "        'coordinates': [[\n",
    "            [ul[1], ul[0]],\n",
    "            [lr[1], ul[0]],\n",
    "            [lr[1], lr[0]],\n",
    "            [ul[1], lr[0]],\n",
    "            [ul[1], ul[0]],\n",
    "        ]],\n",
    "    }\n",
    "    geometry = ogr.CreateGeometryFromJson(json.dumps(geojson))\n",
    "    \n",
    "    # download a dem for the AOI\n",
    "    prepare_dem_vrt(analysis_directory/\"dem.vrt\", geometry)\n",
    "\n",
    "    # convert the vrt to a geotiff\n",
    "    latlon_name = f\"{fnames[0].parent}/latlon_dem.tif\"\n",
    "    utm_name = f\"{str(fnames[0].parent/fnames[0].parent.relative_to(fnames[0].parents[1]))}_dem.tif\"\n",
    "    gdal.Translate(destName=latlon_name, \n",
    "                   srcDS=str(analysis_directory/\"dem.vrt\"), \n",
    "                   format='GTiff')\n",
    "    \n",
    "    # set AREA_OR_POINT to Point\n",
    "    dem = gdal.Open(latlon_name, gdal.GA_Update)\n",
    "    dem.SetMetadataItem('AREA_OR_POINT', 'Point')\n",
    "    dem = None\n",
    "    \n",
    "    # Align dem pixels to an integer resolution value\n",
    "    gdal.Warp(utm_name, latlon_name, \n",
    "              dstSRS=f'EPSG:{utm}', srcSRS='EPSG:4326', \n",
    "              xRes=resolution, yRes=resolution, targetAlignedPixels=True)\n",
    "    Path(latlon_name).unlink()\n",
    "\n",
    "    # Align geotiffs to an integer resolution value\n",
    "    fnames = list(analysis_directory.glob('*/*.tif'))\n",
    "    fnames.sort()\n",
    "    for fname in fnames:\n",
    "        gdal.Warp(str(fname), str(fname), \n",
    "                  dstSRS=f'EPSG:{utm}', srcSRS=f'EPSG:{utm}', \n",
    "                  xRes=resolution, yRes=resolution, targetAlignedPixels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check the InSAR metadata files and add `Earth radius at nadir` and `Spacecraft height`, if missing**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if sarviews_data:\n",
    "    md_txt_files = analysis_directory.glob(f\"*/*.md.txt\")\n",
    "    for file in md_txt_files:\n",
    "        file.unlink()\n",
    "\n",
    "    metadata_files = list(analysis_directory.glob('*/*.txt'))\n",
    "    for file in metadata_files:\n",
    "        parameter_found = False\n",
    "        with open(file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if 'Earth radius at nadir' in line:\n",
    "                    parameter_found = True\n",
    "                    break\n",
    "        if not parameter_found:   \n",
    "            with open(file, 'a') as f:\n",
    "                lines = ['Spacecraft height: 693000.0\\n', 'Earth radius at nadir: 6337286.638938101\\n']\n",
    "                f.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a HyP3 object and authenticate:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "if hyp3_data:\n",
    "    hyp3 = HyP3(prompt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List projects containing active InSAR products and select one:**\n",
    "\n",
    "Your HyP3 InSAR project should include DEMs, which are available as options when submitting a HyP3 project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3_data:\n",
    "    my_hyp3_info = hyp3.my_info()\n",
    "    active_projects = dict()\n",
    "\n",
    "    for project in my_hyp3_info['job_names']:\n",
    "        batch = Batch()\n",
    "        batch = hyp3.find_jobs(name=project, job_type='INSAR_GAMMA').filter_jobs(running=False, include_expired=False)\n",
    "        if len(batch) > 0:\n",
    "            active_projects.update({batch.jobs[0].name: batch})\n",
    "\n",
    "    if len(active_projects) > 0:\n",
    "        display(Markdown(\"<text style='color:darkred;'>Note: After selecting a project, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "        display(Markdown(\"<text style='color:darkred;'>Otherwise, you will rerun this code cell.</text>\"))\n",
    "        print('\\nSelect a Project:')\n",
    "        project_select = asfn.select_parameter(active_projects)\n",
    "\n",
    "    display(project_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select a date range of products to download:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3_data:\n",
    "    jobs = project_select.value\n",
    "\n",
    "    display(Markdown(\"<text style='color:darkred;'>Note: After selecting a date range, you should select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "    display(Markdown(\"<text style='color:darkred;'>Otherwise, you may simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Date Range:')\n",
    "    dates = asfn.get_job_dates(jobs)\n",
    "    date_picker = asfn.gui_date_picker(dates)\n",
    "    display(date_picker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the selected date range and remove products falling outside of it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3_data:\n",
    "    date_range = asfn.get_slider_vals(date_picker)\n",
    "    date_range[0] = date_range[0].date()\n",
    "    date_range[1] = date_range[1].date()\n",
    "    print(f\"Date Range: {str(date_range[0])} to {str(date_range[1])}\")\n",
    "    jobs = asfn.filter_jobs_by_date(jobs, date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gather the available paths and orbit directions for the remaining products:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3_data:\n",
    "    display(Markdown(\"<text style='color:darkred;'><text style='font-size:150%;'>This may take some time for projects containing many jobs...</text></text>\"))\n",
    "    jobs = asfn.get_paths_orbits(jobs)\n",
    "    paths = set()\n",
    "    orbit_directions = set()\n",
    "    for p in jobs:\n",
    "        paths.add(p.path)\n",
    "        orbit_directions.add(p.orbit_direction)\n",
    "    display(Markdown(f\"<text style=color:blue><text style='font-size:175%;'>Done.</text></text>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Select a path:**\n",
    "\n",
    "This notebook does not currently support merging InSAR products in multiple paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3_data:\n",
    "    display(Markdown(\"<text style='color:darkred;'>Note: After selecting a path, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "    display(Markdown(\"<text style='color:darkred;'>Otherwise, you will simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Path:')\n",
    "    path_choice = asfn.select_parameter(paths)\n",
    "    display(path_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the selected flight path/s:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3_data:\n",
    "    flight_path = path_choice.value\n",
    "    if flight_path:\n",
    "        if flight_path:\n",
    "            print(f\"Flight Path: {flight_path}\")\n",
    "        else:\n",
    "            print('Flight Path: All Paths')\n",
    "    else:\n",
    "        print(\"WARNING: You must select a flight path in the previous cell, then rerun this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select an orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3_data:\n",
    "    if len(orbit_directions) > 1:\n",
    "        display(Markdown(\"<text style='color:red;'>Note: After selecting a flight direction, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "        display(Markdown(\"<text style='color:red;'>Otherwise, you will simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Flight Direction:')\n",
    "    direction_choice = asfn.select_parameter(orbit_directions, 'Direction:')\n",
    "    display(direction_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the selected orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3_data:\n",
    "    direction = direction_choice.value\n",
    "    print(f\"Orbit Direction: {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter jobs by path and orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3_data:\n",
    "    jobs = asfn.filter_jobs_by_path(jobs, [flight_path])\n",
    "    jobs = asfn.filter_jobs_by_orbit(jobs, direction)\n",
    "    print(f\"There are {len(jobs)} products to download.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the products, unzip them into a directory named after the product type, and delete the zip files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3_data:\n",
    "    print(f\"\\nProject: {jobs.jobs[0].name}\")\n",
    "    project_zips = jobs.download_files(analysis_directory)\n",
    "    for z in project_zips:\n",
    "        asfn.asf_unzip(str(analysis_directory), str(z))\n",
    "        z.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Confirm Presence of a DEM, Azimuth Angle Map, and Incidence Angle Map\n",
    "\n",
    "- These are optional addon products for HyP3, which are necessary for MintPy\n",
    "- Both azimuth and incidence angle maps are included with HyP3 jobs when the `Include Inc. Angle Map` option is selected\n",
    "- DEMs are not included in SARVIEWS events, so we download and mosaic one in Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dems = list(analysis_directory.glob('*/*dem.tif'))\n",
    "az_angle_maps = list(analysis_directory.glob('*/*lv_phi.tif'))\n",
    "inc_angle_maps = list(analysis_directory.glob('*/*lv_theta.tif'))\n",
    "\n",
    "if len(dems) > 0:\n",
    "    print(\"Success: Found at least 1 DEM.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Failed to find at least 1 DEM. \\\n",
    "    \\nYou will not be able to successfully run a MintPy time-series unless your reorder your HyP3 project \\\n",
    "with DEMS or provide one from another source.\")\n",
    "    \n",
    "if len(az_angle_maps) > 0 and len(inc_angle_maps) > 0:\n",
    "    print(\"Success: Found at least 1 Azimuth Angle Map and 1 Incidence Angle Map.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Failed to find at least 1 Azimuth Angle Map or Incidence Angle Map. \\\n",
    "    \\nYou will not be able to successfully run a MintPy time-series unless your reorder your HyP3 project \\\n",
    "with 'Include Inc. Angle Map' option selected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Subset the Stack and Cleanup Unused Files\n",
    "\n",
    "**Delete unneeded files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pattern in [\"xml\",\"png\",\"kmz\",\"md.txt\"]:\n",
    "    unneeded_files = analysis_directory.glob(f\"*/*.{pattern}\")\n",
    "    for file in unneeded_files:\n",
    "        file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideOutput": true
   },
   "source": [
    "**Subset the timeseries to your AOI:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = list(analysis_directory.glob(f'*/*_amp.tif'))\n",
    "merge_paths = \"\"\n",
    "\n",
    "for pth in amp:\n",
    "    merge_paths = f\"{merge_paths} {pth}\"\n",
    "\n",
    "full_scene = analysis_directory/\"full_scene.tif\"\n",
    "if full_scene.exists():\n",
    "    full_scene.unlink()\n",
    "gdal_command = f\"gdal_merge.py -o {full_scene} {merge_paths}\"\n",
    "!{gdal_command}\n",
    "\n",
    "image_file = f\"{analysis_directory}/raster_stack.vrt\"\n",
    "!gdalbuildvrt -separate $image_file -overwrite $full_scene\n",
    "img = gdal.Open(image_file)\n",
    "rasterstack = img.ReadAsArray()\n",
    "\n",
    "aoi = asfn.AOI_Selector(rasterstack, 7.5, 7.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the AOI plot's x,y values to georaphic coordinates:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geotrans = img.GetGeoTransform()\n",
    "\n",
    "def geolocation(x, y, geotrans):\n",
    "    return [geotrans[0]+x*geotrans[1], geotrans[3]+y*geotrans[5]]\n",
    "\n",
    "try:\n",
    "    ul = geolocation(aoi.x1, aoi.y1, geotrans)\n",
    "    lr = geolocation(aoi.x2, aoi.y2, geotrans)\n",
    "    print(f\"AOI Corner Coordinates:\")\n",
    "    print(f\"upper left corner: {ul}\")\n",
    "    print(f\"lower right corner: {lr}\")\n",
    "except TypeError:\n",
    "    print('TypeError')\n",
    "    display(Markdown(f'<text style=color:red>This error may occur if an AOI was not selected.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that the square tool icon in the AOI selector menu is <b>NOT</b> the selection tool. It is the zoom tool.</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crop the stack to the AOI and reproject to lat-lon:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = list(analysis_directory.glob('*/*.tif'))\n",
    "fnames.sort()\n",
    "    \n",
    "\n",
    "for i, fname in enumerate(fnames):\n",
    "    clip = fname.parent/f\"{fname.stem}_clip.tif\"\n",
    "    gdal.Translate(destName=str(clip), srcDS=str(fname), projWin=[ul[0], ul[1], lr[0], lr[1]])\n",
    "    gdal.Warp(str(clip), str(clip), dstSRS='EPSG:4326', dstNodata=0)\n",
    "    fname.unlink() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove any subset scenes containing no data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = list(analysis_directory.glob('*/*.tif*'))\n",
    "fnames = [str(f) for f in fnames]\n",
    "fnames.sort()\n",
    "\n",
    "removed = []\n",
    "for f in fnames:\n",
    "    if not \"dem\" in str(f):\n",
    "        raster = gdal.Open(f)\n",
    "        if raster:\n",
    "            band = raster.ReadAsArray()\n",
    "            if np.count_nonzero(band) < 1:\n",
    "                Path(f).unlink()\n",
    "                removed.append(f)\n",
    "\n",
    "if len(removed) == 0:\n",
    "    print(\"No Geotiffs were removed\")\n",
    "else:\n",
    "    print(f\"{len(removed)} GeoTiffs removed:\")\n",
    "    for f in removed:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Proceed to the MintPy Time-Series Notebook\n",
    "\n",
    "**Run the code cell below for a link to the MintPy_Time_Series_From_Prepared_Data_Stack notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f'<text style=color:green> Open <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/notebooks/SAR_Training/English/Master/MintPy_Time_Series_From_Prepared_Data_Stack.ipynb\" target=\"_blank\"> MintPy_Time_Series_From_Prepared_Data_Stack.ipynb </a>to run an InSAR time-series analysis on your data using MintPy</text>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"Calibri\" size=\"2\"> <i>Prepare_HyP3_InSAR_Stack_for_MintPy.ipynb - Version 1.1.1 - September 2021\n",
    "    <ul>\n",
    "        <li>Download and DEM for SARVIEWS events and align to stack</li>\n",
    "        <li>Add missing \"Earth radius at nadir\" and \"Spacecraft height\" metadata parameters to SARVIEWS events</li>\n",
    "        <li>Remove subset tifs containing no data</li>\n",
    "    </ul>\n",
    "    </i>\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insar_analysis [conda env:.local-insar_analysis]",
   "language": "python",
   "name": "conda-env-.local-insar_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
